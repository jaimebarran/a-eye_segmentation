{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnUNet\n",
    "Prepare input data to launch nnUNet into the CHUV cluster.\n",
    "\n",
    "Check this repo: https://github.com/PeterMcGor/nnUNet/tree/docker_and_singularity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read subjects from txt (train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, random, tempfile, glob, logging, math, csv\n",
    "\n",
    "txt_dir = f'/mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/3D_multilabel/experiment_0/'\n",
    "data_dir = '/mnt/sda1/Repos/a-eye/a-eye_segmentation/atlas_registration/ANTs/' # train and val\n",
    "\n",
    "# Train subjects\n",
    "train_file = txt_dir + 'train_subjects.txt' # train subjects list file\n",
    "train_subs = [] # empty list to read list from a file\n",
    "# open file and read the content in a list\n",
    "with open(train_file, 'r') as fp:\n",
    "    for line in fp:\n",
    "        # remove linebreak from a current name\n",
    "        # linebreak is the last character of each line\n",
    "        x = line[:-1]\n",
    "        # add current item to the list\n",
    "        train_subs.append(x)\n",
    "# display list\n",
    "print(train_subs)\n",
    "\n",
    "# Val subjects\n",
    "val_file = txt_dir + 'val_subjects.txt' # val subjects list file\n",
    "val_subs = [] # empty list to read list from a file\n",
    "# open file and read the content in a list\n",
    "with open(val_file, 'r') as fp:\n",
    "    for line in fp:\n",
    "        # remove linebreak from a current name\n",
    "        # linebreak is the last character of each line\n",
    "        x = line[:-1]\n",
    "        # add current item to the list\n",
    "        val_subs.append(x)\n",
    "# display list\n",
    "print(val_subs)\n",
    "\n",
    "# Test\n",
    "file = txt_dir + 'test_subjects.txt' # test subjects list file\n",
    "test_subs = [] # empty list to read list from a file\n",
    "# open file and read the content in a list\n",
    "with open(file, 'r') as fp:\n",
    "    for line in fp:\n",
    "        # remove linebreak from a current name\n",
    "        # linebreak is the last character of each line\n",
    "        x = line[:-1]\n",
    "        # add current item to the list\n",
    "        test_subs.append(x)\n",
    "# display list\n",
    "print(test_subs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy subjects to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "output_dir = f'/mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet_raw_data/Task313_Eye/'\n",
    "imagesTr_path = f'{output_dir}imagesTr/'\n",
    "labelsTr_path = f'{output_dir}labelsTr/'\n",
    "imagesTs_path = f'{output_dir}imagesTs/'\n",
    "\n",
    "# Adding val subjects into train set for the nnUNet\n",
    "train_val_subs = train_subs + val_subs\n",
    "\n",
    "# Lengths of sets\n",
    "len_test_subs = len(test_subs)\n",
    "print(f'Number of test subjects: {len_test_subs}')\n",
    "len_train_val_subs = len(train_val_subs)\n",
    "print(f'Number of train subjects: {len_train_val_subs}')\n",
    "\n",
    "# Toggle one subject (to test) or the entire set\n",
    "TOGGLE_ENTIRE_SET = True\n",
    "\n",
    "# Test set\n",
    "for j in range(len_test_subs):\n",
    "    \n",
    "    # Input path\n",
    "    input_t1 = data_dir + 'a123/' + test_subs[j] + '/input/' + test_subs[j] + '_T1_aff.nii.gz'\n",
    "    print(f'Path T1: {input_t1}')\n",
    "\n",
    "    # New name\n",
    "    new_name_im = f'AEye_{j+1:03d}_0000.nii.gz'\n",
    "    print(f'New name image T1: {new_name_im}')\n",
    "\n",
    "    # Copy\n",
    "    shutil.copy(input_t1, f'{imagesTs_path}{new_name_im}')\n",
    "\n",
    "    if j==0 and TOGGLE_ENTIRE_SET==False:\n",
    "        break\n",
    "\n",
    "# Train set\n",
    "for i in range(len_train_val_subs):\n",
    "    \n",
    "    # Input paths\n",
    "    input_t1 = data_dir + 'a123/' + train_val_subs[i] + '/input/' + train_val_subs[i] + '_T1_aff.nii.gz'\n",
    "    print(f'Path T1: {input_t1}')\n",
    "    input_labels = data_dir + 'a123/' + train_val_subs[i] + '/input/' + train_val_subs[i] + '_labels_aff.nii.gz'\n",
    "    print(f'Path labels: {input_labels}')\n",
    "\n",
    "    # New name\n",
    "    new_name_im = f'AEye_{i+1+len_test_subs:03d}_0000.nii.gz'\n",
    "    print(f'New name image T1: {imagesTr_path}{new_name_im}')\n",
    "    new_name_lab = f'AEye_{i+1+len_test_subs:03d}.nii.gz'\n",
    "    print(f'New name label: {labelsTr_path}{new_name_lab}')\n",
    "\n",
    "    # Copy\n",
    "    shutil.copy(input_t1, f'{imagesTr_path}{new_name_im}')\n",
    "    shutil.copy(input_labels, f'{labelsTr_path}{new_name_lab}')\n",
    "\n",
    "    if i==0 and TOGGLE_ENTIRE_SET==False:\n",
    "        break\n",
    "\n",
    "# Dealing with files in that folder\n",
    "for f in glob.glob(f'{imagesTr_path}/*0001.nii.gz'):\n",
    "    os.remove(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunet.dataset_conversion.utils import generate_dataset_json\n",
    "\n",
    "# Output paths\n",
    "output_dir = f'/mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet_raw_data/Task313_Eye/'\n",
    "imagesTr_path = f'{output_dir}imagesTr/'\n",
    "labelsTr_path = f'{output_dir}labelsTr/'\n",
    "imagesTs_path = f'{output_dir}imagesTs/'\n",
    "\n",
    "generate_dataset_json(output_file=os.path.join(output_dir, 'dataset.json'), imagesTr_dir=imagesTr_path, imagesTs_dir=imagesTs_path,\n",
    "                        modalities=('T1w',), labels={0: 'background', 1: 'lens', 2: 'globe', 3: 'optic nerve',\n",
    "                        4: 'intraconal fat', 5: 'extraconal fat', 6: 'lateral rectus muscle', \n",
    "                        7: 'medial rectus muscle', 8: 'inferior rectus muscle', 9: 'superior rectus muscle'},\n",
    "                        dataset_name='AEyeDataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker nnUnet local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Use decathlon datasets\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /home/jaimebarranco/Desktop/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_convert_decathlon_task -i /home/jaimebarranco/Desktop/nnUNet/nnUNet_raw_data/Task02_Heart -p NUM_PROCESSES (8 for example)\n",
    "\n",
    "# Verify dataset integrity\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /home/jaimebarranco/Desktop/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_plan_and_preprocess -t 2 --verify_dataset_integrity\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_plan_and_preprocess -t 313 --verify_dataset_integrity\n",
    "\n",
    "# To enter the Docker machine (-it for interactive and bash for the machine)\n",
    "sudo docker run --gpus device=0 -it --shm-size=10gb -v /home/jaimebarranco/Desktop/nnUNet:/opt/nnUNet_resources petermcgor/nnunet:0.0.1 bash\n",
    "\n",
    "# Training locally\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /home/jaimebarranco/Desktop/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_MYTASK FOLD --npz\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_train 3d_fullres nnUNetTrainerV2 Task313_Eye 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHUV cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Login\n",
    "ssh -o \"ServerAliveInterval 60\" ja4922@hpc1-login.chuv.ch\n",
    "\n",
    "# Transfer files to CHUV cluster\n",
    "scp -r /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnunet_slurm.sh ja4922@hpc1-login.chuv.ch:/home/ja4922/AEye/\n",
    "\n",
    "# Transfer files from CHUV cluster\n",
    "scp -r ja4922@hpc1-login.chuv.ch:/data/bach/AEye/nnUNet/nnUNet_trained_models/nnUNet/3d_fullres/Task313_Eye/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/progress.png /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/\n",
    "\n",
    "# Training on HPC\n",
    "sbatch jobfile\n",
    "sbatch /home/ja4922/AEye/nnunet_slurm.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=nn_bs2_f0\n",
    "#SBATCH --chdir=/home/ja4922/AEye\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jaime.barranco-hernandez@chuv.ch\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=1-23:59:00\n",
    "#SBATCH --output=eye_3d_f0_out_%N.%j.%a.out\n",
    "#SBATCH --error=eye_3d_f0_err_%N.%j.%a.err\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=64gb\n",
    "## Apparently these lines are needed for GPU execution\n",
    "#SBATCH --account rad\n",
    "#SBATCH --partition rad\n",
    "#SBATCH --gres=gpu:1\n",
    "\n",
    "singularity run --bind /data/bach/AEye/nnUNet:/opt/nnunet_resources --nv docker://petermcgor/nnunet:0.0.1 nnUNet_train 3d_fullres nnUNetTrainerV2 Task313_Eye 0 --npz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference (on HPC): create jobfile and send it to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=nn_inf\n",
    "#SBATCH --chdir=/home/ja4922/AEye\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=jaime.barranco-hernandez@chuv.ch\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time=1-23:59:00\n",
    "#SBATCH --output=eye_3d_inf_out_%N.%j.%a.out\n",
    "#SBATCH --error=eye_3d_inf_err_%N.%j.%a.err\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=64gb\n",
    "## Apparently these lines are needed for GPU execution\n",
    "#SBATCH --account rad\n",
    "#SBATCH --partition rad\n",
    "#SBATCH --gres=gpu:1\n",
    "singularity run --bind /data/bach/AEye/nnUNet:/opt/nnunet_resources --nv docker://petermcgor/nnunet:0.0.1 nnUNet_find_best_configuration -t 313"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import numpy as np\n",
    "# from nnunet.paths import preprocessing_output_dir\n",
    "\n",
    "# Paths\n",
    "task_name = 'Task313_Eye'\n",
    "preprocessing_output_dir = f'/mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet_preprocessed/'\n",
    "\n",
    "# if it breaks upon loading the plans file, make sure to run the Task120 dataset conversion and\n",
    "# nnUNet_plan_and_preprocess first!\n",
    "plans_fname = join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "plans = load_pickle(plans_fname)\n",
    "plans['plans_per_stage'][0]['batch_size'] = 1\n",
    "# plans['plans_per_stage'][0]['patch_size'] = np.array((512, 512))\n",
    "# plans['plans_per_stage'][0]['num_pool_per_axis'] = [7, 7]\n",
    "\n",
    "# because we changed the num_pool_per_axis, we need to change conv_kernel_sizes and pool_op_kernel_sizes as well!\n",
    "# plans['plans_per_stage'][0]['pool_op_kernel_sizes'] = [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]]\n",
    "# plans['plans_per_stage'][0]['conv_kernel_sizes'] = [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]\n",
    "\n",
    "# for a network with num_pool_per_axis [7,7] the correct length of pool kernel sizes is 7 and the length of conv\n",
    "# kernel sizes is 8! Note that you can also change these numbers if you believe it makes sense. A pool kernel size\n",
    "# of 1 will result in no pooling along that axis, a kernel size of 3 will reduce the size of the feature map\n",
    "# representations by factor 3 instead of 2.\n",
    "\n",
    "# save the plans under a new plans name. Note that the new plans file must end with _plans_2D.pkl!\n",
    "save_pickle(plans, join(preprocessing_output_dir, task_name, 'nnUNetPlansv2.1_bs1_plans_3D.pkl'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run file with changed batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# command\n",
    "nnUNet_train 3d nnUNetTrainerV2 313 0 -p nnUNetPlansv2.1_bs1\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_train 3d_fullres nnUNetTrainerV2 Task313_Eye 0 -p nnUNetPlansv2.1_bs1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# nnUNet_find_best_configuration will print a string to the terminal with the inference commands you need to use\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_find_best_configuration -m 3d_fullres -t 313\n",
    "\n",
    "# determine post processing. Note that you do not have to run nnUNet_determine_postprocessing if you use nnUNet_find_best_configuration. nnUNet_find_best_configuration will do that for you.\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_determine_postprocessing  -m 3d_fullres -t 313 \n",
    "\n",
    "# inference\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_predict -i /opt/nnunet_resources/nnUNet_raw_data/Task313_Eye/imagesTs -o /opt/nnunet_resources/nnUNet_inference/nnUNet_inference_labeled_dataset -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t Task313_Eye"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-labeled dataset INFERENCE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy subjects to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, random, tempfile, glob, logging, math, csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Input path\n",
    "data_dir = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_nifti/'\n",
    "\n",
    "# Output paths\n",
    "output_dir = f'/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_nifti_nnunet/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "test_subjects = [sub for sub in sorted(Path(data_dir).rglob('*.nii.gz'))]\n",
    "print(f'num subs: {len(test_subjects)}')\n",
    "\n",
    "# Test set\n",
    "for i in range(len(test_subjects)):\n",
    "    \n",
    "    # Input path\n",
    "    input_t1 = test_subjects[i]\n",
    "    print(f'Path T1: {input_t1}')\n",
    "\n",
    "    # New name\n",
    "    sub_name = str(test_subjects[i]).split('/')[8]\n",
    "    new_name_im = f'AEye_{sub_name}_0000.nii.gz'\n",
    "    print(f'New name image T1: {new_name_im}')\n",
    "\n",
    "    # Copy\n",
    "    shutil.copy(input_t1, f'{output_dir}{new_name_im}')\n",
    "\n",
    "    # if i==0:\n",
    "    #     break\n",
    "\n",
    "# Dealing with files in that folder\n",
    "# for f in glob.glob(f'{imagesTr_path}/*0001.nii.gz'):\n",
    "#     os.remove(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run nnunet inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_predict -i /opt/nnunet_resources/nnUNet_inference/non_labeled_dataset_nifti_nnunet -o /opt/nnunet_resources/nnUNet_inference/nnUNet_inference_non_labeled_dataset -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t Task313_Eye\n",
    "\n",
    "# test (changing the postprocessing.json file to enable all the classes)\n",
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_predict -i /opt/nnunet_resources/nnUNet_inference/test/input -o /opt/nnunet_resources/nnUNet_inference/test/output -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t Task313_Eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flipped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# function to switch the orientation of a .nii image from left to right to right to left\n",
    "def switch_orientation(image):\n",
    "    nifti_image = nib.load(image)\n",
    "    image_data = nifti_image.get_fdata()\n",
    "    flipped_data = np.flip(image_data, axis=1)\n",
    "    flipped_image = nib.Nifti1Image(flipped_data, affine=nifti_image.affine)\n",
    "    return flipped_image\n",
    "\n",
    "input_image = \"/home/jaimebarranco/Desktop/2022160100001_0000.nii.gz\"\n",
    "output_image = switch_orientation(input_image)\n",
    "nib.save(output_image, \"/mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet/nnUNet_inference/temp_inference/input/sub-01_0000.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo docker run --gpus device=0 --shm-size=10gb -v /mnt/sda1/Repos/a-eye/a-eye_segmentation/deep_learning/nnUNet/nnUNet:/opt/nnunet_resources petermcgor/nnunet:0.0.1 nnUNet_predict -i /opt/nnunet_resources/nnUNet_inference/temp_inference/input -o /opt/nnunet_resources/nnUNet_inference/temp_inference/output -tr nnUNetTrainerV2 -ctr nnUNetTrainerV2CascadeFullRes -m 3d_fullres -p nnUNetPlansv2.1 -t Task313_Eye"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('3dmultilabel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "236677abf66ddcf33094279068f0bab187d6786b2da11104f264e332fab0dfcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
